# -*- coding: utf-8 -*-
"""03_pytorch_MultiClassClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r9LWxOhlvsIlxEnbf3Ic__l3E6x61y9e

### Creating a toy multi-class dataset
"""

import torch
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# Set hyperparameters for data creation

NUM_CLASSES = 4
NUM_FEATURES = 2
RANDOM_SEED = 42


# 1. Create multi-class data
x_blob, y_blob = make_blobs(n_samples = 5000,
                            n_features = NUM_FEATURES,
                            centers = NUM_CLASSES,
                            cluster_std = 1.5,
                            random_state = 40)

# 2. Turn data into Tensors (since scikit learn uses numpy)

X_blob, y_blob = torch.from_numpy(x_blob).type(torch.float), torch.from_numpy(y_blob).type(torch.float)

# 3. Train Test Split
X_train, X_test, y_train, y_test = train_test_split(X_blob, y_blob, test_size = 0.3, random_state = 42)


# 4. Plot data
plt.figure(figsize = (10, 7))
plt.scatter(X_blob[:, 0], X_blob[:, 1], c = y_blob, cmap = plt.cm.RdYlBu)
plt.show()

df = pd.DataFrame({'X1': X_blob[:,0],
                   'X2': X_blob[:,1],
                   'y' : y_blob})

torch.unique(y_blob)

"""## Building a Multi-class classification model"""

# Create device agnostic code
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

import torch.nn as nn

class BlobModelMultiClass(nn.Module):
  def __init__(self, in_feat, out_feat, h1_units = 8, h2_units = 5):
    super().__init__()
    # self.layer1 = nn.Linear(in_features = in_feat, out_features = h1_units)
    # self.layer2 = nn.Linear(in_features = h1_units, out_features = h2_units)
    # self.layer3 = nn.Linear(in_features = h2_units, out_features = out_feat)

    # self.relu = nn.ReLu()

    self.layer_stack = nn.Sequential(
        nn.Linear(in_features = in_feat, out_features = h1_units),
        nn.ReLU(),
        nn.Linear(in_features = h1_units, out_features = h2_units),
        nn.ReLU(),
        nn.Linear(in_features = h2_units, out_features = out_feat)
    )

  def forward(self, X):
    # return self.layer3(self.relu(self.layer2(self.relu(self.layer1))))
    return self.layer_stack(X)

model_ = BlobModelMultiClass(in_feat = 2, out_feat = 4).to(device)

model_

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params = model_.parameters(),
                             lr = 0.1)

"""### Getting pred probs for multi-class model in PyTorch

> Logits(raw) → Pred Probs → Pred  Labels

"""

y_test[:10]

model_.eval()
with torch.inference_mode():
  y_logits = model_(X_test)
y_logits

y_pred_probs = torch.softmax(y_logits, dim = 1)
y_pred_probs

y_preds = torch.argmax(y_pred_probs, dim = 1)
y_preds[:10]

loss = loss_fn(y_logits, y_test.type(torch.LongTensor))

"""## Evaluation Metrics for Classification

NEED TO IMPORT TORCHMETRICS (after installing)

* Accuracy - how many does our model got right, out of 100 samples (best useful for balanced classes, not for imbalanced) `torchmetrics.Accuracy()` or `sklearn.metrics.accuracy_score()`
* Precision - Number of actually postive out of all predicted postives `torchmetrics.Precision()`

> Higher precision leads to less false positives

* Recall - Number of correctly predicted positive out of actual positives `torchmetrics.Recall()`

> Higher recall leads to less false negatives
* F1- Score - Combination of Precision and Recall `torchmetrics.F1Score()`
* Confusion Matrix - `torchmetrics.ConfusionMatrix()`
* Classification Report - `sklearn.metrics.classification_report()`

*If needed to implement own metric, subclass the `Metric` class.*
"""

# Calculate accuracy (a classification metric)
def accuracy_fn(y_true, y_pred):
    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal
    acc = (correct / len(y_pred)) * 100
    return acc

"""## Traning loop and testing"""

torch.manual_seed(42)
epochs = 200

for epoch in range(epochs):
  model_.train()
  y_logits = model_.forward(X_train)
  y_pred_probs = torch.softmax(y_logits, dim = 1)
  y_pred = torch.argmax(y_pred_probs, dim = 1)

  loss = loss_fn(y_logits, y_train.type(torch.LongTensor))

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  train_acc = accuracy_fn(y_train, y_pred)

  ### Testing
  model_.eval()
  with torch.inference_mode():
    test_logits = model_(X_test)
    test_preds = torch.argmax(torch.softmax(test_logits, dim = 1), dim = 1)

    test_loss = loss_fn(test_logits, y_test.type(torch.LongTensor))
    test_acc = accuracy_fn(y_test, test_preds)

  if epoch % 20 == 0:
    print(f"Epoch: {epoch} | Train loss: {loss} | Train Acc: {train_acc} | Test loss: {test_loss} | Test Acc: {test_acc}")

def plot_decision_boundary(model, X, y):
  # X, y = X.to_numpy(), y.to_numpy()
  x1_min, x1_max = X[:,0].min(), X[:,0].max()
  x2_min, x2_max = X[:,1].min(), X[:,1].max()
  x1x1, x2x2 = np.meshgrid(np.linspace(x1_min, x1_max, 201), np.linspace(x2_min, x2_max, 201))

  # features
  feat_x1 = np.ravel(x1x1)
  feat_x2 = np.ravel(x2x2)
  feat_ = torch.from_numpy(np.column_stack((feat_x1, feat_x2))).float()

  # make predictions
  model.eval()
  with torch.inference_mode():
    preds = model(feat_)
    preds = torch.argmax(torch.softmax(preds, dim = 1), dim = 1)

  preds = preds.reshape(x1x1.shape)
  plt.contourf(x1x1, x2x2, preds, cmap = plt.cm.RdYlBu, alpha = 0.7)
  plt.scatter(X[:,0], X[:,1], c = y, s = 40, cmap = plt.cm.RdYlBu)
  plt.xlim(x1x1.min(), x1x1.max())
  plt.ylim(x2x2.min(), x2x2.max())

plt.figure(figsize = (10, 6))
plt.subplot(1,2,1)
plt.title("Train")
plot_decision_boundary(model_, X_train, y_train)
plt.subplot(1,2,2)
plt.title("Test")
plot_decision_boundary(model_, X_test, y_test)

!pip install torchmetrics

from torchmetrics import Accuracy, Precision, Recall

torch_acc = Accuracy(task="multiclass", num_classes=4)
torch_acc(test_preds, y_test)

from torchmetrics import ConfusionMatrix
confmat = ConfusionMatrix(task="multiclass", num_classes=4)
confmat(test_preds, y_test)

