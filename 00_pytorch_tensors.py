# -*- coding: utf-8 -*-
"""00_pytorch_tensors.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14eaWR9ZbWEYrxOf31JVdRYyVkXi56O_b

Have a happy learning in PyTorch!!!
"""

import torch
print(torch.__version__)

"""###Introduction to tensors

creating tensors

represent multidimensional data

"""

#scalar

scalar = torch.tensor(7)
scalar
#go through documentation

scalar.ndim

#get tensor back as Python int
scalar.item()

#vector
vector = torch.tensor([6,8])
vector

vector.ndim
#number of sq brackets

vector.shape
#2 elements

#Matrix
MATRIX = torch.tensor([[2,3],[4,5]])
MATRIX

print(MATRIX.ndim)
print(MATRIX.shape)
MATRIX[1]

#tensor
TENSOR = torch.tensor([[[1,2,3],[2,3,4],[8,9,5]]])
TENSOR, TENSOR.ndim, TENSOR[0], TENSOR[0,1], TENSOR.shape #1 3/3 tensor,

"""Random Tensors

Important because, many NN learn is that they start with tensors full of random numbers and then adjust those random numbers for better represent the data

start with random numbers ⟶ look at data ⟶ update random numbers ⟶ look at data ⟶ update random numbers
"""

# create a Random tensor of size (3,4)
rand_tensor = torch.rand(3,4)
#go through the doc
rand_tensor

print(rand_tensor.ndim)
print(rand_tensor.shape)

# create a random tensor with similar shape to an image tensor
random_image_size_tensor = torch.rand(size=(224, 224, 3))
#[224, 224, 3] ([height, width, color_channels]).
random_image_size_tensor.shape, random_image_size_tensor.ndim

"""Zeroes and Ones"""

# create a tensor of all zeroes
zeros = torch.zeros(3,4)
# create a tensor of all ones
ones = torch.ones(3,4)

zeros, ones, ones.dtype

"""### Tensor like, ranges"""

one_to_ten = torch.arange(1,23,2)
some_zeros = torch.zeros_like(input=one_to_ten)

one_to_ten, some_zeros

"""###Tensor datatypes

**Note:** Tensor datatypes is one of the 3 big errors you'll run into with PyTorch and DL
1. Tensors not right datatype
2. Tensors not right shape(while mat multiplication)
3. Tensors not on the right device
"""

fl_32 = torch.tensor([3.0,6.0,9.0],dtype=None, device = None)
fl_32.dtype

fl_16 = fl_32.type(torch.float16)
fl_16

t = fl_16*fl_32
t.dtype

int_32 = torch.tensor([3,6,9], dtype=torch.int32)
int_32

int_32*fl_16

some_tensor = torch.rand(3,4)
print(some_tensor)
print(f"Shape of the Tensor: {some_tensor.shape}")
print(f"DataType of the Tensor: {some_tensor.dtype}")
print(f"Device tensor is on: {some_tensor.device}")

"""### Manipulating Tensors(tensor operations)

Tensor operations include
* addition
* subtraction
* multiplication
* division
* matrix multiplication
"""

tensor = torch.tensor([1,2,3])
print(tensor + 10)
print(tensor*10)
# tryout inbuilt functions
torch.mul(tensor,2)

# element wise multiplication
print(tensor,"*",tensor)
print(f"Equals: {tensor*tensor}")

#matrix multiplication)

print(f"Matrix multiplication equals: {torch.matmul(tensor,tensor)}")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# value = 0
# for i in range(len(tensor)):
#   value+= tensor[i]*tensor[i]
# print(value)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(tensor,tensor)

"""### one of the most COMMON ERRORS in DL

2 main rules that performing matmul need to satisfy:
1. The **inner dimensions** must match
2. The resulting matrix has the shape of the **outer dimensions**
"""

# shapes for matrix multiplication
tensor_A = torch.tensor([[1,2],
                         [3,4],
                         [5,6]])
tensor_B = torch.tensor([[7,10],
                         [8,11],
                         [9,12]])
#torch.mm(tensor_A,tensor_B) can't be multiplied
#torch.mm is short form of torch.matmul
print(tensor_A)
print(tensor_B,tensor_B.shape)

tensor_B.T, tensor_B.T.shape

torch.mm(tensor_A, tensor_B.T),torch.mm(tensor_A, tensor_B.T).shape

"""## Find the min, max, mean, sum, etc (tensor aggregation)"""

x = torch.arange(3,105, 10)
x, x.dtype

torch.min(x), x.min()

torch.max(x), x.max()

#torch.mean inputs only datatype of float32
#hence we need to convert type of x
torch.mean(x.type(torch.float32)), x.type(torch.float32).mean()

# sum
torch.sum(x), x.sum()

# index position of min and max values in the tensor
x.argmin(), x.argmax()

"""### Reshaping, stacking, squeezing and unsqueezing tensors
* Reshaping - reshapes an input tensor to a defined shape
* View - return a view of an input tensor of certain shape but keep the same memory as th original tensor
* Stacking - combine multiple tensors on top of each other (vstack) or side by side (hstack)
* Squeeze - removes all 1 dimensions from a tensor
* Unsqueeze - add a 1 dimension to a target tensor
* Permute - Return a view of the input with dimensions permuted(swapped) in a certain way
"""

# Let's create a tensor
x = torch.arange(1.0,10.0)
x

x_reshaped = x.reshape(3,3)
x_reshaped

x_reshape = x.reshape(1,9)
x_reshape

z = x.view(1,9)
#here as z shares the same memory as x, any changes made in z will also show up in the original x
print(z)
z[:,0]=5.0
print(z)
print(x)

#stacking the tensors
x_stacked = torch.stack((x,x,x,x),dim=1)
x_stacked, x_stacked.shape

torch.vstack((x,x,x,x)), torch.vstack((x,x,x,x)).shape

torch.hstack((x,x,x,x)), torch.hstack((x,x,x,x)).shape

print(f"Previous tensor: {x_reshape}\n")
print(f"Previous shape: {x_reshape.shape}\n")

#remove extra dimensions from the x_reshaped
x_squeezed = x_reshape.squeeze()
print(f"New Tensor: {x_squeezed}\n")
print(f"New shape: {x_squeezed.shape}")

#torch.unsqueeze adds a single dim to a target at a specific dim
print(f"Previous tensor: {x_squeezed}\n")
print(f"Previous shape: {x_squeezed.shape}\n")

#add an extra dimension with unsqueeze
x_unsqueezed = x_squeezed.unsqueeze(dim=1) #add dim at dim 1
print(f"New Tensor: {x_unsqueezed}\n")
print(f"New shape: {x_unsqueezed.shape}\n\n")

x_unsqueezed = x_squeezed.unsqueeze(dim=0) #add dim at dim 0
print(f"New Tensor: {x_unsqueezed}\n")
print(f"New shape: {x_unsqueezed.shape}")

x_new = x_reshape.reshape(3,3)
print(f"Previous tensor: {x_new}\n")
print(f"Previous shape: {x_new.shape}\n")

#remove extra dimensions from the x_new
x_squeezed = x_new.squeeze()
print(f"New Tensor: {x_squeezed}\n")
print(f"New shape: {x_squeezed.shape}")

#torch.unsqueeze adds a single dim to a target at a specific dim
print(f"Previous tensor: {x_squeezed}\n")
print(f"Previous shape: {x_squeezed.shape}\n")

#add an extra dimension with unsqueeze
print("Unsqueezing in Dim 2")
x_unsqueezed = x_squeezed.unsqueeze(dim=2) #add dim at dim 2
print(f"New Tensor: {x_unsqueezed}\n")
print(f"New shape: {x_unsqueezed.shape}\n")
print("---------------------------------------\n")

print("Unsqueezing in Dim 1\n")
x_unsqueezed = x_squeezed.unsqueeze(dim=1) #add dim at dim 1
print(f"New Tensor: {x_unsqueezed}\n")
print(f"New shape: {x_unsqueezed.shape}\n")
print("---------------------------------------\n")

print("Unsqueezing in Dim 0\n")
x_unsqueezed = x_squeezed.unsqueeze(dim=0) #add dim at dim 0
print(f"New Tensor: {x_unsqueezed}\n")
print(f"New shape: {x_unsqueezed.shape}")
print("---------------------------------------")

"""Permute is commonly used with images"""

# torch.permute rearranges the dimensions of a target tensor in a specified order
x = torch.randint(2,10,(2,4,3))
print(f"The size of the tensor is: {x.shape}\n")
print(f"The tensor is: {x}\n\n")

#now rearrange the shape as you need by changing in the index of the dim
print("\n-------------New permutation 2,0,1---------------------\n")
x_permuted = x.permute(2,0,1)
print(f"The size of the permuted tensor is: {x_permuted.shape}\n")
print(f"The permuted tensor is: {x_permuted}")

# permuted and original tensors share the same memory
# try changing the value in permuted on and check the original one

x_permuted[:,:,0] = 0
print(f"The new permuted tensor is: {x_permuted}\n")
print(f"Now let's see what happened in the original tensor: {x}")

x = torch.randint(2,10,(2,4,3))

print("\n-------------New permutation 1,2,0---------------------\n")
x_permuted = x.permute(1,2,0)
print(f"The size of the permuted tensor is: {x_permuted.shape}\n")
print(f"The permuted tensor is: {x_permuted}")

# permuted and original tensors share the same memory
# try changing the value in permuted on and check the original one

x_permuted[:,:,0] = 0
print(f"The new permuted tensor is: {x_permuted}\n")
print(f"Now let's see what happened in the original tensor: {x}")

import torch
x = torch.arange(1,10).reshape(1,3,3)
x, x.shape

x[0]

x[0,0] #or x[0][0]

# let's index on most inner bracket
x[0][2][1]

# you can use ":" to select all of a target dimension
x[:,0]

x[:,:,0]

x[0,2,:]



import torch
x = torch.arange(1,19).reshape(3,3,2)
x, x.shape

x[0]

x[0,0] #or x[0][0]

# let's index on most inner bracket
x[0][2][1]

# you can use ":" to select all of a target dimension
x[:,0]

x[:,:,0]

x[0,2,:]

# Indexing (selecting data from tensors)
# similar to numpy

"""### PyTorch and NumPy

NumPy is a popular scientific Python numerical computing library
And because of this, PyTorch has functionality to interact with it.
* Data in NumPy, want in PyTorch tensor -> `torch.from_numpy(ndarray)`
* PyTorch tensor -> NumPy -> `torch.Tensor.numpy()`
"""

#numpys default dtype is float 64 whereas torch's tensor default dtype is float32
import numpy as np

array = np.arange(1.0,10.0)
tensor = torch.from_numpy(array)
array, tensor

#change the value in array -> what affect will it have on tensor
array = array+1
array, tensor
#it didn't change i.e both have different memory space

# Tensor to NumPy array
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
numpy_tensor
#default dtype of array will be that of tensor now

"""## Reproducability (trying to take random out of random)

In short how s neural network learns:
`start with random numbers -> tensor operations -> update random numbers to try and make them better representation of the data -> again -> again -> again..`

To reduce the randomness in neural networks and PyTorch comes the concept of a **random seed**.

Essentially what the random seed does is "flavour" the randomness.

A pseudorandom number generator's number sequence is completely determined by the seed: thus, if a pseudorandom number generator is reinitialized with the same seed, it will produce the same sequence of numbers.
"""

# Let's make some random but reproducilble tensors

#set the random seed
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tensor_A = torch.rand(3,4)

#the below line of code works only for one random gen
torch.manual_seed(RANDOM_SEED)
random_tensor_B = torch.rand(3,4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_B==random_tensor_A)
#hence we get the same random numbers or data

"""## Running tensors and PyTorch objects on the GPUs (and making computations easier)"""

!nvidia-smi

#check for GPU access with PyTorch
import torch
torch.cuda.is_available()

# Set-up device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

#count number of devices
torch.cuda.device_count()

"""Putting tensors on the GPU"""

import torch
# create a tensor (default on the CPU)
tensor = torch.tensor([1,2,3])

# Tensor not on GPU
print(tensor, tensor.device)

# Move tensor to GPU (if available)
tensor_on_gpu = tensor.to(device)
tensor_on_gpu

"""Moving tensors back to the CPU"""

# If tensor is on GPU, can't transform it to NumPy
tensor_on_gpu.numpy()

# To fix the GPU tensor with NumPy issue, we can first set it to the CPU
tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu

tensor_on_gpu